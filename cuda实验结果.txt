实验配置: 模型=/data/models/Qwen3-14B/, 并发数=10, 输入长度约=1024, 生成长度=1024
正在加载模型和分词器...
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.07it/s]
正在准备 10 路并发输入数据...
输入数据准备完成。每个请求实际输入token数: 1024 (总输入: 10240 tokens)
pynvml初始化成功，将监控GPU。
正在进行GPU预热...
GPU预热完成。

--- 开始正式测试 ---

阶段1: 测试近似Prefill性能 (生成1个新token)
  Prefill阶段（及首个token生成）耗时: 14.8183 秒
  Prefill阶段输入Token总数: 10240
  Prefill阶段（近似）吞吐量: 691.04 tokens/秒
  Prefill阶段平均GPU利用率: 99.29%
  Prefill阶段平均GPU显存使用: 63847.93 MB

阶段2: 测试完整生成性能 (生成 1024 个新token)
  完整生成 (1024个新tokens/请求) 耗时: 150.8190 秒
  完整生成阶段新生成Token总数: 10240
  完整生成阶段平均GPU利用率: 99.28%
  完整生成阶段平均GPU显存使用: 64758.54 MB

  近似后续Decode阶段 (除去首个token后):
    耗时: 136.0007 秒
    生成Token数: 10230
    吞吐量: 75.22 tokens/秒

--- 实验结果总结与分析 ---
模型: /data/models/Qwen3-14B/, 并发数: 10
输入长度/请求 (目标): ~1024 tokens, (实际padding后): 1024 tokens
生成长度/请求 (目标): 1024 tokens, (实际): 1024 tokens

性能对比:
  1. 近似Prefill阶段 (处理输入 10240 tokens 并生成首批 10 tokens):
     - 耗时: 14.8183 s
     - 输入吞吐量: 691.04 input tokens/s
     - 平均GPU利用率: 99.29%

  2. 近似后续Decode阶段 (生成后续 10230 tokens):
     - 耗时: 136.0007 s
     - 输出吞吐量: 75.22 output tokens/s
     - (参考完整生成阶段的平均GPU利用率: 99.28%)

分析:
  观察到Decode阶段的输出吞吐量 (75.22 tokens/s) 显著低于Prefill阶段的输入吞吐量 (691.04 tokens/s)。
  Prefill吞吐量约是Decode吞吐量的 9.19 倍。
  这表明，在Decode阶段，尽管GPU可能仍然保持一定的利用率（尤其是在高并发下），但每个token的处理速度变慢了。这主要是因为：
    a. 自回归特性：每个token的生成依赖于前一个，限制了单序列内的并行度。
    b. 内存带宽限制：频繁读写KV Cache可能成为瓶颈，尤其当上下文变长、并发请求多时。
    c. 计算密度较低：相比Prefill阶段大规模处理输入，Decode阶段每个step的计算量相对较小。
  这种效率差异正是PD（Prefill-Decode）分离优化（如使用专门的kernel、PagedAttention、Continuous Batching等）所要解决的核心问题，
  旨在提高Decode阶段的效率，从而提升整体的LLM服务吞吐和响应速度。

实验结束，资源已清理。
