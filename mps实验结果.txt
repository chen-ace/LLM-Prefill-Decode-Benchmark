检测到Apple Silicon MPS 后端可用，将使用 'mps' 设备。
实验配置: 模型=gpt2, 并发数=5, 输入长度约=256, 生成长度=256, 设备=mps
正在加载模型和分词器...
Tokenizer未设置pad_token，已自动设为eos_token_id: 50256
正在准备 5 路并发输入数据...
输入数据准备完成。每个请求实际输入token数: 255 (总输入: 1275 tokens)
------------------------------
Apple Silicon (MPS) GPU活动监控提示:
此脚本不包含自动MPS GPU利用率跟踪。您可以通过以下方式手动观察：
1. 打开“活动监视器” -> 菜单栏“窗口” -> “GPU历史记录”。
2. 在终端运行: sudo powermetrics --samplers gpu_power -i 1000
   (按 Ctrl+C 停止 powermetrics)。
请在脚本的关键阶段（如模型生成时）观察GPU活动。
------------------------------
正在进行设备预热...
设备预热完成。

--- 开始正式测试 ---
提示：请手动开始观察MPS GPU活动。

阶段1: 测试近似Prefill性能 (生成1个新token)
提示：请手动结束观察MPS GPU活动。
  Prefill阶段（及首个token生成）耗时: 0.2431 秒
  Prefill阶段输入Token总数: 1275
  Prefill阶段（近似）吞吐量: 5245.42 tokens/秒

阶段2: 测试完整生成性能 (生成 256 个新token)
提示：请手动开始观察MPS GPU活动。
提示：请手动结束观察MPS GPU活动。
  完整生成 (256个新tokens/请求) 耗时: 34.6408 秒
  完整生成阶段新生成Token总数: 1280

  近似后续Decode阶段 (除去首个token后):
    耗时: 34.3978 秒
    生成Token数: 1275
    吞吐量: 37.07 tokens/秒

--- 实验结果总结与分析 ---
模型: gpt2, 并发数: 5, 设备: mps
输入长度/请求 (目标): ~256 tokens, (实际padding后): 255 tokens
生成长度/请求 (目标): 256 tokens, (实际): 256 tokens

性能对比:
  1. 近似Prefill阶段 (处理输入 1275 tokens 并生成首批 5 tokens):
     - 耗时: 0.2431 s
     - 输入吞吐量: 5245.42 input tokens/s
     - (请回顾此阶段手动观察的MPS GPU活动)

  2. 近似后续Decode阶段 (生成后续 1275 tokens):
     - 耗时: 34.3978 s
     - 输出吞吐量: 37.07 output tokens/s
     - (请回顾此阶段手动观察的MPS GPU活动)

分析:
  观察到Decode阶段的输出吞吐量 (37.07 tokens/s) 显著低于Prefill阶段的输入吞吐量 (5245.42 tokens/s)。
  Prefill吞吐量约是Decode吞吐量的 141.51 倍。
  这表明，在Decode阶段，即使GPU（在MPS上通过活动监视器观察）可能仍然保持一定的活动，但每个token的处理速度变慢了。这主要是因为：
    a. 自回归特性：每个token的生成依赖于前一个，限制了单序列内的并行度。
    b. 内存带宽限制：频繁读写KV Cache可能成为瓶颈，尤其当上下文变长、并发请求多时。      (在MPS上，统一内存架构的特性可能与独立显存的NVIDIA GPU表现不同，但带宽仍是因素)
    c. 计算密度较低：相比Prefill阶段大规模处理输入，Decode阶段每个step的计算量相对较小。
  这种效率差异正是PD（Prefill-Decode）分离优化所要解决的核心问题，
  旨在提高Decode阶段的效率，从而提升整体的LLM服务吞吐和响应速度。

实验结束，资源已清理。